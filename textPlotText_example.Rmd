---
title: "example PlotFuncs"
author: "AlexGu at Lund University"
date: "`r Sys.Date()`"
output:
  rmdformats::readthedown:
    self_contained: true
    thumbnails: true
    lightbox: true
    gallery: false
    highlight: tango
---

```{r setup, include=FALSE}
options(warn=-1)
knitr::opts_chunk$set(echo = TRUE)

```

This tutorial will use the <b><big>Language_based_assessment_data_8</big></b> in Text package to generate a plot.

# Import packages and functions

```{r import the plotting functions, echo=TRUE}

if (FALSE){
  # install packages needed for punctuation restoring. In a future Text version, this is not necessary.
  reticulate::conda_install(envname="textrpp_condaenv", c("protobuf","sentencepiece"), forge=FALSE, pip=TRUE)
}

library(tidyverse)
library(text)
library(topics)

reticulate::source_python("fullStopCorrt.py")
reticulate::source_python("utils.py")
source(
  "paragraph_viz.R", # plotting functions
  encoding=localeToCharset()
)

options(future.globals.maxSize = 2.0 * 1e9)

# supported models: bert, roberta base models
nameLLM <- "bert-base-uncased"
sign <- get_subword_sign(nameLLM)
tokenizer <- getTokenizer(nameLLM)
```

# Import data

```{r data import, echo=TRUE}
data <- dep_wor_data
text_set <- "Deptext"
scale_total <- "PHQ9tot"
data_length <- 500
print(nrow(data)) # 40 records
```
# Restore punctuations 

```{r restore punctuations, echo=TRUE}
# Choose harmonytexts as the visualization target in this example.
data <- data[,c(text_set, scale_total)]

# Only needed if one finds the missing of puncuations in the text.
# !!!! After running this code, a restart of the R console is needed. Plan to fix in the future.
if (FALSE){
  data[,"harmonytexts"] <- sentsMarker(data[,"harmonytexts"])
  .rs.restartR()
}

```

# Get embeddings

```{r get embed, echo=TRUE}
embed1 <- createEmbeddings(
                    data[2:data_length, text_set],
                    model=nameLLM,
                    device = "gpu", # cpu, gpu, mps
                    dim_name = FALSE, # For prediction of new texts, this param must be FALSE,
                    "both"
)
```

# Train a model to predict the target

```{r regression, echo=TRUE}
theModels <- trainLanguageModel(embed1, # embedding
                       data[2:data_length, scale_total], # scale values, target
                       "all", # options: "token", "sentence", "passage", "all", # "all" include all the previous models
                       tokenizer,
                       modelName = nameLLM
)
```

```{r save the regression model, echo=TRUE}
# Save the models for future use since the training is a bit slow.
saveRDS(theModels, paste0("topic_dep_model.rds"))
#theModels <- readRDS("model.rds")
```

# Use the model to predict new texts
```{r Get embed of pred texts and predict, echo=TRUE}
source(
  "paragraph_viz.R", # plotting functions
  encoding=localeToCharset()
)
toPredEmbed <- createEmbeddings(data[1,text_set],
                         model=nameLLM,
                         dim_name=FALSE, 
                         device="gpu",
                         includeCLSSEP="both")
```


# Plot preparation

```{r Plot_preparation, echo=TRUE}
source(
  "paragraph_viz.R", # plotting functions
  encoding=localeToCharset()
)
output <- predictLanguage(toPredEmbed, theModels, "all", tokenizer, nameLLM)
```

```{r Plot_preparation, echo=TRUE}
source(
  "paragraph_viz.R", # plotting functions
  encoding=localeToCharset()
)
print(output)
```

```{r Plot_preparation, echo=TRUE}
source(
  "paragraph_viz.R", # plotting functions
  encoding=localeToCharset()
)
colored <- createColoredTibble(output, limits = c(0, 27))
```

```{r Plot_preparation, echo=TRUE}
source(
  "paragraph_viz.R", # plotting functions
  encoding=localeToCharset()
)

plot <- generateDocument(colored, toPredEmbed, nameLLM)
plot
```