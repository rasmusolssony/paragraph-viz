---
title: "paragraph_viz examples"
author: "Nils Broman"
date: "`r Sys.Date()`"
output:
  rmdformats::readthedown:
    self_contained: true
    thumbnails: true
    lightbox: true
    gallery: false
    highlight: tango
---

```{r setup, include=FALSE}
options(warn=1)
knitr::opts_chunk$set(echo = TRUE)

```

# Import packages and functions
```{r import the plotting functions, echo=TRUE}

if (FALSE){
  # install packages needed for punctuation restoring. In a future Text version, this is not necessary.
  reticulate::conda_install(envname="textrpp_condaenv", c("protobuf","sentencepiece"), forge=FALSE, pip=TRUE)
}

library(tidyverse)
devtools::load_all("../text")
library(topics)
library(future)
library(ggplot2)
library(plotly)
library(htmltools)
library(pandoc)

plan(multisession)

reticulate::source_python("fullStopCorrt.py")
reticulate::source_python("utils.py")
source(
  "paragraph_viz.R", # plotting functions
  encoding=localeToCharset()
)

options(future.globals.maxSize = 1.0 * 1e10)

```

# Select model, data and parameters
```{r data import, echo=TRUE}
# supported models: bert, roberta base models, mxbai
nameLLM <- "bert-base-uncased"
#nameLLM <- "mixedbread-ai/mxbai-embed-large-v1"
#nameLLM <- "Alibaba-NLP/gte-large-en-v1.5"
# nameLLM <- "jxm/cde-small-v2"

text_set <- "Deptext"
#scale_total <- "PHQ9tot"
scale_total <- "PHQtot" #for 1000 dataset
# dataset: dep_wor_data
#data <- dep_wor_data
# extended dataset
data <- read_csv("dataset.csv")

data <- data[, c(text_set, scale_total)]
data <- drop_na(data)

data_length <- nrow(data)
# size of train and test sets
test_size <- 50
train_size <- data_length - test_size

model_folder_path <- paste0("models/", nameLLM, "-", text_set, "-", train_size, "-ridge") # nolint: line_length_linter.
embeddings_folder_path <- paste0("models/", nameLLM, "-", text_set, "-", train_size)
save_model <- TRUE
save_embeddings <- TRUE
force_train <- FALSE
```

# Create embeddings
```{r train, echo=TRUE}
if(!file.exists(paste0(embeddings_folder_path, "/embeddings.rds")) || force_train){

  embed <- createEmbeddings(
                      data[(test_size+1):data_length, text_set],
                      model=nameLLM,
                      device = "gpu", # cpu, gpu, mps
                      dim_name = FALSE, # For prediction of new texts, this param must be FALSE,
                      embedSentences = FALSE, # Should be false for paragraph level training
                      "both",
                      trust_remote_code = TRUE,
  )

  if (save_embeddings) {
    if(!dir.exists(file.path(embeddings_folder_path))){
      dir.create(file.path(embeddings_folder_path))
    }
    saveRDS(embed, paste0(embeddings_folder_path, "/embeddings.rds"))
  }
} else {
  embed <- readRDS(paste0(embeddings_folder_path, "/embeddings.rds"))
}
```

# Train Model / Load Model
```{r train, echo=TRUE}
if(!file.exists(paste0(model_folder_path, "/models.rds")) || force_train){

  theModels <- trainLanguageModel(embed, # embedding
                        data[(test_size+1):data_length, scale_total], # scale values, target
                        "paragraph", # options: "token", "sentence", "paragraph", "all", # "all" include all the previous models
                        modelName = nameLLM,
                        mixture = 1
  )

  if (save_model) {
    if(!dir.exists(file.path(model_folder_path))){
      dir.create(file.path(model_folder_path))
    }
    saveRDS(theModels, paste0(model_folder_path, "/models.rds"))
  }

} else {
  theModels <- readRDS(paste0(model_folder_path, "/models.rds"))
}
```

# Predict and plot
```{r pred and plot, echo=TRUE}
source(
  "paragraph_viz.R", # plotting functions
  encoding=localeToCharset()
)
paragraphs_to_plot <- 1:50

toPredEmbed <- createEmbeddings(data[paragraphs_to_plot,text_set],
                         model=nameLLM,
                         dim_name=FALSE,
                         device="gpu",
                         embedSentences = TRUE,
                         includeCLSSEP="both",
                         trust_remote_code = TRUE)

output <- predictLanguage(toPredEmbed, theModels, "all", nameLLM,)

print("Pearson correlation: ")
print(pearsonCorrelation(output$paragraphs$predicted_value, data[paragraphs_to_plot,scale_total]))


#plot <- generateDocument(output, toPredEmbed, data[paragraph_to_plot, scale_total], nameLLM, limits = c(0, 27), filePath = paste0(model_folder_path, "/plot_paragraph_", paragraph_to_plot, ".html"))
#plot
```
```{r}
source(
  "paragraph_viz.R", # plotting functions
  encoding=localeToCharset()
)
plot <- generateDocument(output, data[paragraphs_to_plot, scale_total], limits = c(0, 27), filePath = paste0(model_folder_path, "/plot_paragraphs_", paragraphs_to_plot[1], "-", paragraphs_to_plot[length(paragraphs_to_plot)], ".html"))
plot
```
```{r}
source(
  "paragraph_viz.R", # plotting functions
  encoding=localeToCharset()
)
paragraphs_to_plot <- 1:50

toPredEmbedShapley <- createEmbeddings(data[paragraphs_to_plot,text_set],
                         model=nameLLM,
                         dim_name=FALSE,
                         device="gpu",
                         embedSentences = TRUE,
                         includeCLSSEP="both",
                         divideByAll = TRUE,
                         trust_remote_code = TRUE)

normalizedOutput <- predictLanguage(toPredEmbedShapley, theModels, "all", nameLLM,
                          divideByAll = TRUE)

```

```{r}
source(
  "paragraph_viz.R", # plotting functions
  encoding=localeToCharset()
)

zeroEmbeddings <- tibble(
  tokens = "zero",  # First column
  !!!setNames(as.list(rep(0, 1024)), paste0("Dim", 1:1024))  # Generate 1024 columns with names Dim1 to Dim1024
)

beta <- theModels$paragraphModel$final_model$fit$fit$fit$a0[[1]]
beta_old <- predict(zeroEmbeddings, theModels[["paragraphModel"]], nameLLM)
print(beta)
#print(normalizedOutput)
shapleyPredictions <- getShapleyPredictions(normalizedOutput, beta)
```

```{r}
source(
  "paragraph_viz.R", # plotting functions
  encoding=localeToCharset()
)
plot <- generateDocument(shapleyPredictions, data[paragraphs_to_plot, scale_total], limits = c(0, 27), shapley = TRUE, filePath = paste0(model_folder_path, "/plot_paragraphs_", paragraphs_to_plot[1], "-", paragraphs_to_plot[length(paragraphs_to_plot)], "-shapley",".html"))
plot
```

```{r}
print(output$paragraphs$predicted_value)
print(data[paragraphs_to_plot,scale_total] %>% as.list())
#print(length(output$paragraphs$predicted_value))
#print(length((data[paragraphs_to_plot,scale_total] %>% unname() %>% as.list())))
plot(x = output$paragraphs$predicted_value, y = (data[paragraphs_to_plot,scale_total] %>% as.list())$PHQtot)
abline(0,1)
```

```{r}
source(
  "text_annotation/text_highlight.R",
  encoding=localeToCharset()
)
texts <- output
generateShiny(texts)

```

```{r}

source(
  "paragraph_viz.R", # plotting functions
  encoding=localeToCharset()
)

saveRDS(output, "texts.rds")

```